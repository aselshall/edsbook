{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46061d42-f95a-4fe2-9e83-800abe4f42b9",
   "metadata": {},
   "source": [
    "# Lesson 15: AI Coding Assistance\n",
    "\n",
    "This lesson was generated with assistance from Jupter AI using ChatGPT 3.5 Turbo.\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/aselshall/eds/HEAD)\n",
    "\n",
    "**Note**: This lesson is based on the older version of Jupyter AI with Python 3.11. The new version of Jupyter AI has more seamless integration with Jupyter Notebook and much easier to use compared to the older version. This ai magic commands are not that much needed in the new version.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad2f84-e023-4464-9294-e8038f07679d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Using an AI code assistant, we will explore the use of generative AI models, mainly language models (LMs) such as ChatGPT, in Jupyter. This is to perform various coding tasks such as generating, completing, debugging, explaining, formatting, and optimizing Python codes. By the end of this lesson, you will be able to:\n",
    "- explain the pros and cons of generative AI in enhancing your Python learning and productivity  \n",
    "- use a generative AI model such as ChatGPT in Jupyter through an AI code assistant such as [Jupyter AI](https://github.com/jupyterlab/jupyter-ai)\n",
    "- chat with and perform coding tasks in Jupyter with a generative AI model of your choice\n",
    "- utilize multiple generative AI models in an open crowdsourced environment with Chatbot Arena (*optional*)\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fa6a1c0-869a-4d3a-9ed1-2451c43d1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_ai              \n",
    "from ai_assistant import api_key  #Import api_key module  \n",
    "api_key.set_API_key('OPENAI')     #Set API key for selected Provider: 'OPENAI' and 'ANTHROPIC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260ced2-b97d-4a2c-96bb-90231b1a7ab6",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ee847-e8fd-4dd5-b323-e5cafadf1b05",
   "metadata": {},
   "source": [
    "### 1.1 Generative AI to enhance your Python learning and productivity\n",
    "\n",
    "Generative AI refers to artificial intelligence capabilities that can generate new content and insights automatically. In this lesson, we will explore how generative AI within the Jupyter notebook environment can **augment human capabilities**, and **enhance learning and productivity**.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/images/mathexam.png\" width=\"800\">\n",
    "\n",
    "AI - Language models (LMs) have different capabilities with respect to reasoning, coding, mathematics, and language comprehension. This figure shows proficiency in mathematics (GSM8K Score) and model’s generalisation abilities (Exam Score) on the Hungarian National High School Exam (Image Credit: [DeepSeek-LLM](https://github.com/deepseek-ai/DeepSeek-LLM))"
   ]
  },
  {
   "attachments": {
    "bf40960c-4306-4b80-b3e2-34d1d45dec63.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEzCAYAAAD+VdWxAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nHkXSAAAAAElFTkSuQmCC\n"
    }
   },
   "cell_type": "markdown",
   "id": "23f12e92-52e4-4678-b21c-2e3e7688e744",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2 AI code assistant\n",
    "\n",
    "AI code assistants such as [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) leverage generative AI models from different providers such as [OpenAI : ChatGPT](https://chat.openai.com/)  within an IDE environment such as [JupyterLab](https://jupyter.org/).\n",
    "An AI code assistant will provide: \n",
    "- prompt engineering with respect to your programming language, \n",
    "- context-aware code suggestions, completions, debugging, formatting, explaination and generation\n",
    "- chat user-interface to ask question and get help on related topics such as installation troubleshooting\n",
    "- and many more\n",
    "\n",
    "This is to improvde learning and productivitiy. \n",
    "\n",
    "![flowchart.png](attachment:bf40960c-4306-4b80-b3e2-34d1d45dec63.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf7da09-ae9d-42ae-a2c2-be1bcee6ca07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3 Gallery of AI code assistants \n",
    "\n",
    "Selecting an AI code assistant depends on factors such as language support, integration with preferred IDEs, customization options, accuracy in suggestions, real-time feedback, resource efficiency, and cost considerations as shown in the table. \n",
    "\n",
    "| AI Code Assistant | Providers: Models | Compatible IDE | Pros | Cons | Use-fee |\n",
    "|---|---|---|---|---|---|\n",
    "| [Jupyter AI]((https://github.com/jupyterlab/jupyter-ai)) | AI21, [Anthropic](https://claude.ai/chats), AWS, Cohere, [Hugging Face](https://huggingface.co), NVIDIA, [OpenAI](https://openai.com/) and more (via third-party plugins) | [JupyterLab](https://jupyter.org/) | 1. Seamless Jupyter integration<br> 2. Multi-feature chat user-interface <br> 3. Supports text-embedding | 1. Not user-friendly<br> 2. Chat user-interface may not function <br> 3. Authentication of API KEY for each notebook <br> 4. Requires plugins for each provider | Free but generative AI models may not be free|\n",
    "| [ChatGPT Jupyter AI Assistant](https://github.com/TiesdeKok/chat-gpt-jupyter-extension)| [OpenAI: ChatGPT]([OpenAI](https://openai.com/) | [JupyterLab](https://jupyter.org/) | Impressive and user-friendly code assistant features | 1. No longer maintained and advise to switch to Jupyter AI<br>2. Code assistant features may not function | Free but generative AI models may not be free|\n",
    "| [Amazon CodeWhisperer](https://aws.amazon.com/codewhisperer/) | Amazon: In-house AI models |  [JupyterLab](https://jupyter.org/), [PyCharm](https://www.jetbrains.com/pycharm/), and [VSCode](https://code.visualstudio.com/]) | 1. Seamless Jupyter integration<br> 2. Real-time feedback | Installation build error may occure| Individual Tier is free for individual use |\n",
    "| [GitHub Copilot](https://github.com/features/copilot) | OpenAI: Codex | [PyCharm](https://www.jetbrains.com/pycharm/), and [VSCode](https://code.visualstudio.com/]) | 1. Powerful and mature code assistant<br> 2 . Context-aware suggestions<br> 3. Automated code refactoring | No Jupyter integration  | Free for students and educators|\n",
    "\n",
    "Here we will use Jupyter AI, but you can also experiment with other ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d88735-1eab-40da-b482-5b4bb5c237e1",
   "metadata": {},
   "source": [
    "## 2. Jupyter AI Extension\n",
    "\n",
    "This section is modefied from [jupyter-ai documentation](https://github.com/jupyterlab/jupyter-ai). You can also check the YouTube video [AWS re:Invent 2023 - Jupyter AI](https://youtu.be/nDoojNaRhPE) where the developers introduce this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6ada1-3ca6-4cea-85fe-0bb5632fac1f",
   "metadata": {},
   "source": [
    "### 2.1 Overview\n",
    "\n",
    "[Jupyter AI](https://github.com/jupyterlab/jupyter-ai) connects generative AI models with Jupyter notebooks, which can enhance your learning and productivity. Specifically, Jupyter AI:\n",
    "1. turns your notebook into generative AI playground\n",
    "2. provides chat user-interface in JupyterLab for chatting with your generative AI model\n",
    "3. supports a wide range of generative model providers including AI21, Anthropic, AWS, Cohere, Hugging Face, NVIDIA, and OpenAI \n",
    "4. allows users to run generative AI models on their own machines through GPT4All rather than relying on cloud-based services.\n",
    "\n",
    "In this section we will learn the first two points with focus on ChatGPT3.5 Turbo of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163f583-72c7-4145-babe-57e2cbc138cf",
   "metadata": {},
   "source": [
    "### 2.2 Installing Jupyter AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a0418-8dc9-4ecb-989d-347ad3758d10",
   "metadata": {},
   "source": [
    "#### 2.2.1 Installation Steps\n",
    "\n",
    "Steps to install Jupyter AI:\n",
    "1. Open an Anaconda Prompt (Anaconda3) or Anaconda Prompt (Miniconda3)\n",
    "2. It is not a bad idea to update your pip before installing a new package\n",
    "```code\n",
    "pip install --upgrade pip\n",
    "```\n",
    "3. Then you can install Python Jupyter AI with pip:\n",
    "```code\n",
    "pip install jupyter-ai\n",
    "```\n",
    "Alternatively, you can use install this extension with a conda. Details on installing and using Jupyter AI can be found on [Jupyter AI official documentation](https://jupyter-ai.readthedocs.io/en/latest/users/index.html).  The above steps should work for windows and linux users. For mac users, you need to do more steps as shown on [jupyter-ai GitHub repository](https://github.com/jupyterlab/jupyter-ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3f513-143b-47aa-aef4-1c2735ad29c1",
   "metadata": {},
   "source": [
    "#### 2.2.2 Installation Troubeshooting\n",
    "\n",
    "The Chat UI on the left menu may not work you will get this error message:\n",
    "```error\n",
    "There seems to be a problem with the Chat backend, please look at the JupyterLab server logs or contact your administrator to correct this problem.\n",
    "```\n",
    "You might need to install few extra packages such as `langchain_nvidia_ai_endpoints` and `cohere`, and **restart your computer**. Check this [stackoverflow post](https://stackoverflow.com/questions/78008875/jupyter-ai-there-seems-to-be-a-problem-with-the-chat-backend/78023497#78023497) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5874ce-d08c-434a-a435-b9435c3942fe",
   "metadata": {},
   "source": [
    "### 2.3 Loading Jupyter AI magic commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ddc4d-e37b-4a3f-a555-75db50f00046",
   "metadata": {},
   "source": [
    "To use Jupyter AI, you need enable the `%ai` and `%%ai` magic commands in your notebook.\n",
    "\n",
    "What is a magic command? Ask your LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abe2f944-0d57-4873-9287-f1452a02666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load extension\n",
    "# %load_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451bd4c-7ba9-4efe-bb45-bf8e75edc1a6",
   "metadata": {},
   "source": [
    "### 2.4 Select provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe896a60-9b03-420a-ae36-dff61a561142",
   "metadata": {},
   "source": [
    "Jupyter AI supports a wide range of model providers and models. To use Jupyter AI with a particular provider, you must install its Python plugins for that provider and set the provider's API key (or other credentials) in your notebook or in the Jupyter AI Chat user-interface (UI) at the left menu.\n",
    "\n",
    "You can view the available providers and models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9184732-fe6c-4fac-8e39-29f640d70fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `azure-chat-openai` | `AZURE_OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `gemini` | `GOOGLE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`gemini:gemini-1.5-pro`</li><li>`gemini:gemini-1.5-flash`</li><li>`gemini:gemini-1.0-pro`</li><li>`gemini:gemini-1.0-pro-001`</li><li>`gemini:gemini-1.0-pro-latest`</li><li>`gemini:gemini-1.0-pro-vision-latest`</li><li>`gemini:gemini-pro`</li><li>`gemini:gemini-pro-vision`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai:babbage-002`</li><li>`openai:davinci-002`</li><li>`openai:gpt-3.5-turbo-instruct`</li></ul> |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai-chat:gpt-3.5-turbo`</li><li>`openai-chat:gpt-3.5-turbo-1106`</li><li>`openai-chat:gpt-4`</li><li>`openai-chat:gpt-4-turbo`</li><li>`openai-chat:gpt-4-turbo-preview`</li><li>`openai-chat:gpt-4-0613`</li><li>`openai-chat:gpt-4-0125-preview`</li><li>`openai-chat:gpt-4-1106-preview`</li><li>`openai-chat:gpt-4o`</li><li>`openai-chat:gpt-4o-2024-11-20`</li><li>`openai-chat:gpt-4o-mini`</li><li>`openai-chat:chatgpt-4o-latest`</li></ul> |\n",
       "| `openrouter` | `OPENROUTER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n",
       "| `openrouter-claude` | `openrouter:anthropic/claude-3.5-sonnet:beta` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: AZURE_OPENAI_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "gemini\n",
       "Requires environment variable: GOOGLE_API_KEY (not set)\n",
       "* gemini:gemini-1.5-pro\n",
       "* gemini:gemini-1.5-flash\n",
       "* gemini:gemini-1.0-pro\n",
       "* gemini:gemini-1.0-pro-001\n",
       "* gemini:gemini-1.0-pro-latest\n",
       "* gemini:gemini-1.0-pro-vision-latest\n",
       "* gemini:gemini-pro\n",
       "* gemini:gemini-pro-vision\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:babbage-002\n",
       "* openai:davinci-002\n",
       "* openai:gpt-3.5-turbo-instruct\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-1106\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-turbo\n",
       "* openai-chat:gpt-4-turbo-preview\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-0125-preview\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "* openai-chat:gpt-4o\n",
       "* openai-chat:gpt-4o-2024-11-20\n",
       "* openai-chat:gpt-4o-mini\n",
       "* openai-chat:chatgpt-4o-latest\n",
       "\n",
       "openrouter\n",
       "Requires environment variable: OPENROUTER_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n",
       "openrouter-claude - openrouter:anthropic/claude-3.5-sonnet:beta\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available LM\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12454720-ff52-49fd-a0bd-d3969c47db56",
   "metadata": {},
   "source": [
    "The environment variable names of API-keys are used when setting up a model.  If multiple variable names are listed for a provider, all must be specified. Check [Jupyter AI offical documentation](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#model-providers) for information about how to use each of the above listed providers. \n",
    "\n",
    "The label `Set` is \n",
    "- ✅ if you provided the API-key for that provider \n",
    "- ❌ if you did not provide the API-key for that provider\n",
    "- N/A if the provider does not require API-key\n",
    "  \n",
    "The label `Models` shows the `provider_name:model_name`. \n",
    "  \n",
    "Aliases are are nicknames for models. For example, typing `chatgpt` is the same as typing `openai-chat:gpt-3.5-turbo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531c743-fbe6-45a7-bcfa-1aaf98816388",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.5 Install provider plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d64cc-c084-4ab8-a34f-a14ad24314ec",
   "metadata": {},
   "source": [
    "You need to select your language model, and you can also select an embedding model. \n",
    "- A language model are typically pre-trained.\n",
    "- An embedding model is used when learning and asking about local data. \n",
    "\n",
    "You can select language model and embedding model through the `Jupyter AI Chat` interface at the left menu or manually.  However, Jupyter AI requires third-party plugins, so before we use a model, we need to install the Python plugins for that model or provider.\n",
    "   \n",
    "The provider that we will select is **OpenAI** that is the developer of ChaTGPT 3.5 Turbo and many other LLMs. For other providers, you need to check the required plugins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23ba265a-c6f0-4e5d-a4ad-56743598124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the command below to install openAI plugins\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95342c9d-74cc-426f-89c2-d324beca40b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.6 Get API-key for the selected provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697c6a2-8080-401d-8bab-793cb4d7564b",
   "metadata": {},
   "source": [
    "To be able to use Jupyter AI for a given notebook, you need to do the environment variable authentication in that notebook using your unique API-key. API-key is a special code that grants users access to the provider services. This code is like a password, so you should not share this code with anyone.\r\n",
    "\n",
    "\n",
    "For this lesson the provider is OpenAI, but you can select any other provider of your choice. You need to [create an OpenAI account](https://openai.com/blog/openai-api) to get OpenAI API key. It will be free for a period of time, and then you can add a credit card number to get charged for your usage of paid services. This [GitHub file](https://github.com/aselshall/eds/blob/main/L/L5/openai_api_key.md) provides information about OpenAI API Key and how to get OpenAI API Key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebc520-40ab-440e-9e86-7197e149a61f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7 Set API-key for the select provider\n",
    "\n",
    "One on the drawbacks of Jupyter AI environment variable authentication through the Chat UI may be insufficent, and you need to do environment variable authentication using your unique API-key in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37c8b-4edd-4066-bb43-e2f773f3d883",
   "metadata": {},
   "source": [
    "#### 3.7.1 Option 1: Reading your API key from the notebook\n",
    "- Pros: Convenient\n",
    "- Cons: Security risk; Ensure to remove your API key before sharing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550e8e1-e076-40bb-b81c-437c04ba1cbc",
   "metadata": {},
   "source": [
    "To set API key from your notebook, you need to import the operating system module `os` and use `os.environ['Environment_variable']= 'API_key'` to set the API for your selected provider. \n",
    "   \n",
    "The example below is for OpenAI provider: \n",
    "```python\n",
    "%load_ext jupyter_ai\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'add your OpenAP API key here'\n",
    "```\n",
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "356765ba-cd59-4778-aafc-4802b0a58b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Jupyter AI extension\n",
    "# %load_ext jupyter_ai \n",
    "# # Set API key\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = 'add your OpenAP API key here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1f27e-1b93-449f-9818-804860ac0e1b",
   "metadata": {},
   "source": [
    "#### 3.7.2 Option 2: Reading API key from an external file \n",
    "- Pros: Enhanced security; Separating the API key into a separate file enables sharing the notebook without exposing the key\n",
    "- Cons: Additional setup required; You must include the code snippet below at the start of each notebook and specify the path to your API key file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc47fc-8b93-4515-b4f3-178d48826121",
   "metadata": {},
   "source": [
    "To set an API key from a file, copy and paste your API-key to a textfile, let us say 'OPENAI_API_KEY.txt'.\n",
    "Then place this code at the beginning of your file. Make sure to change the `file_path_name` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43d11e3f-9212-4e62-b3f7-6213c484ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Jupyter AI extension\n",
    "# %load_ext jupyter_ai\n",
    "\n",
    "# #Read API key from a file\n",
    "# def read_API_Key(file_name):\n",
    "\n",
    "#     # Open the file in read mode \n",
    "#     with open(file_name, 'r') as file:\n",
    "#         # Read the content of the file\n",
    "#         API_key = file.read().strip()  # strip() removes any leading or trailing whitespace\n",
    "#     return API_key\n",
    "\n",
    "# # Set API key\n",
    "# import os\n",
    "# file_path_name = 'ai_assistant/OPENAI_API_KEY.txt'\n",
    "# os.environ['OPENAI_API_KEY'] = read_API_Key(file_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772aa60-01ac-45eb-976a-91ee07ea0e00",
   "metadata": {},
   "source": [
    "#### 3.7.3 Option 3: Create an authentication module \n",
    "- Pros: Versatile solution; Enables authentication for various providers from any location\n",
    "- Cons: Technical proficiency needed; Involves coding skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62b63d-f13e-4f11-bb0c-2a58f20fd5a0",
   "metadata": {},
   "source": [
    "At the beginning of my notebook, I can place these three lines to load AI magic commands and set up environment variable authentication:\n",
    "\n",
    "```python\n",
    "%load_ext jupyter_ai\n",
    "from ai_assistant import api_key  # Import the api_key module\n",
    "api_key.set_API_key('OPENAI')     # Set the API key for the selected provider: 'OPENAI' or 'ANTHROPIC'\n",
    "```\n",
    "\n",
    "By passing the provider name to the `api_key.set_API_key()` function, authentication is performed based on the API key saved in a file. The module is saved somewhere on my machine, but I can import it from anywhere because I structured it as a package with `__init__.py`, rather than just a single Python module file as we covered in a previous lesson.\n",
    "\n",
    "While we have not covered package structures, you can ask your generative AI model to demonstrate how to create a package like `ai_assistant` and develop a module such as `api_key` with the `set_API_key` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d22821f1-6040-4001-9336-c21fdeba6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext jupyter_ai              \n",
    "# from ai_assistant import api_key  #Import api_key module  \n",
    "# api_key.set_API_key('OPENAI')     #Set API key for selected Provider: 'OPENAI' and 'ANTHROPIC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc220187-cb1b-47cc-882a-ded12ccbc1a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.8 Getting help (*optional*)\n",
    "\n",
    "Let use look at the help of Jupyter AI to learn about what Jupyter AI offers and how to use this AI code assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8feea8c-0c56-4031-9c5f-3a0a9be00bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] [MODEL_ID]\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  reset     Clear the conversation transcript.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "  version   Prints Jupyter-AI version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting help\n",
    "%ai --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48bc79-fc1b-497e-bff5-e91b5c05e777",
   "metadata": {},
   "source": [
    "The above help tells us that the magic command\n",
    "```python\n",
    "%%ai [OPTIONS] COMMAND \n",
    "# Or \n",
    "%%ai COMMAND [OPTIONS]\n",
    "```\n",
    "invokes a language model identified by MODEL_ID, with the prompt being contained in all lines after the first. \n",
    "  \n",
    "From  `OPTIONS` , the most important opition is `-f` that allows you to format your model output as `code`, `html`, `image`, `json`, `markdown`, `math`, `md`, or `text`. If this is unclear, it will be clear with an example, so let us see few examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42334270-151b-48a7-b1ea-f9415f3c03c2",
   "metadata": {},
   "source": [
    "You can get help on a specific command. For example, let us get help on `error` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce19fc94-318b-460c-8100-d264e8cd5082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %ai error [OPTIONS] [MODEL_ID]\n",
      "\n",
      "  Explains the most recent error. Takes the same options (except -r) as the\n",
      "  basic `%%ai` command.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "%ai error --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1d127-a066-473d-8e78-fba552a2d13d",
   "metadata": {},
   "source": [
    "### 2.9 Using `%%ai` and `%ai` magic commands (*not very much recommended*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53264acb-65a4-45d0-ac5d-e9df00276976",
   "metadata": {},
   "source": [
    "#### 2.9.1 Using magic command with default format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04aee51-06b5-4cf0-b20c-90eb7d3ec046",
   "metadata": {},
   "source": [
    "Now we want to use ChatGPT-3.5 Turbo to generate a function the finds the minimum value in a list. \n",
    "  \n",
    "Here is our prompt. \n",
    "```\n",
    "Write a function that identifies the minimum value in a list without relying on the built-in min() function.\n",
    "Ensure the function is capable of handling various data types and edge cases.\n",
    "Run at least two test cases to validate the accuracy of the minimum value identification process.\n",
    "```\n",
    "Here is the general format:\n",
    "```python\n",
    "%%ai provider:model [OPTIONS]\n",
    "prompt\n",
    "```\n",
    "In  that case this the provider and model would be `%%ai openai-chat:gpt-3.5-turbo` or simply use the provider-model aliase that is `%%ai chatgpt`. \n",
    "\n",
    "Here is how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90dcd4a4-2a87-488f-bf6d-b5b62a829cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "def find_minimum(lst):\n",
       "    if not lst:\n",
       "        return None\n",
       "\n",
       "    min_val = lst[0]\n",
       "    for i in range(1, len(lst)):\n",
       "        if lst[i] < min_val:\n",
       "            min_val = lst[i]\n",
       "\n",
       "    return min_val\n",
       "\n",
       "# Test case 1\n",
       "print(find_minimum([3, 5, 2, 8, 1]))  # Output: 1\n",
       "\n",
       "# Test case 2\n",
       "print(find_minimum(['apple', 'banana', 'orange', 'pear']))  # Output: 'apple'\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Function that identifies the minimum value in a list without relying on the built-in min() function\n",
    "Function is capable of handling various data types and edge cases\n",
    "Two test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc5f8a-de54-4f82-a006-cab63ead28df",
   "metadata": {},
   "source": [
    "In our prompt, we omitted specifying Python as Jupyter AI will automatically manage the task, providing necessary details like Python version and other relevant information to achieve the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17d97c-156b-4089-8a40-060b7fcae7e8",
   "metadata": {},
   "source": [
    "More importantly, in the above example, the default output is markdown format. We can change this with the argument `[OPTION]`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb78ba-0143-4e8b-a495-a89794951e64",
   "metadata": {},
   "source": [
    "#### 2.9.2. Formatting the output \n",
    "\n",
    "By default the output of an `%%ai` command will be formatted as markdown. You can override this using the `-f` or `--format` argument to your magic command. Valid formats include: `code`, `markdown`,  `math`, `html`, `text`, `json`, and `image` (for Hugging Face Hub's text-to-image models).\n",
    "\n",
    "Repeat the above example using `-f code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfb63775-c448-441e-b1fb-f12fada4fce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai -f code chatgpt \n",
    "Function that identifies the minimum value in a list without relying on the built-in min() function\n",
    "Function is capable of handling various data types and edge cases\n",
    "Two test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a284b-e2d8-47bf-96dc-9492d12fd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimum(lst):\n",
    "    if not lst:\n",
    "        return None\n",
    "\n",
    "    min_val = lst[0]\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] < min_val:\n",
    "            min_val = lst[i]\n",
    "\n",
    "    return min_val\n",
    "\n",
    "# Test case 1\n",
    "print(find_minimum([3, 5, 2, 8, 1]))  # Output: 1\n",
    "\n",
    "# Test case 2\n",
    "print(find_minimum(['apple', 'banana', 'orange', 'pear']))  # Output: 'apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b6f2b55-f115-4a0e-86ca-7371a62be04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-10\n"
     ]
    }
   ],
   "source": [
    "def find_min(lst):\n",
    "    if len(lst) == 0:\n",
    "        return None\n",
    "    \n",
    "    min_val = lst[0]\n",
    "    for val in lst:\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "            \n",
    "    return min_val\n",
    "\n",
    "# Test case 1\n",
    "test_lst1 = [3, 7, 1, 9, 2]\n",
    "print(find_min(test_lst1))  # Output: 1\n",
    "\n",
    "# Test case 2\n",
    "test_lst2 = [-10, 0, 5, -3, 8]\n",
    "print(find_min(test_lst2))  # Output: -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd28fbec-2cfa-46b3-927a-2c0b51006936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def find_min_value(input_list):\n",
    "    if not input_list:\n",
    "        return None\n",
    "    \n",
    "    min_val = input_list[0]\n",
    "    \n",
    "    for item in input_list:\n",
    "        if not isinstance(item, (int, float)):\n",
    "            return None\n",
    "        if item < min_val:\n",
    "            min_val = item\n",
    "    \n",
    "    return min_val\n",
    "\n",
    "# Test cases\n",
    "print(find_min_value([3, 6, 8, 2, 10])) \n",
    "print(find_min_value([]))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45b056b6-07f0-4e01-be28-e6cbeb627e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "def find_min_value(lst):\n",
    "    if not lst:\n",
    "        return None\n",
    "\n",
    "    min_val = lst[0]\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] < min_val:\n",
    "            min_val = lst[i]\n",
    "    \n",
    "    return min_val\n",
    "\n",
    "# Test cases\n",
    "test1 = [3, 5, 1, 9, 2]\n",
    "print(find_min_value(test1))  # Output: 1\n",
    "\n",
    "test2 = ['b', 'c', 'a', 'f']\n",
    "print(find_min_value(test2))  # Output: 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09253c06-62bc-4e7b-98aa-dcf21f0d2266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-50\n"
     ]
    }
   ],
   "source": [
    "def find_min(lst):\n",
    "    if len(lst) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        min_val = lst[0]\n",
    "        for i in range(1, len(lst)):\n",
    "            if lst[i] < min_val:\n",
    "                min_val = lst[i]\n",
    "        return min_val\n",
    "\n",
    "# Test cases\n",
    "print(find_min([3, 5, 1, 9, 2]))  # Output: 1\n",
    "print(find_min([-10, 0, 100, -50]))  # Output: -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78b3a2a3-145b-43ae-8877-c6060230b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "def find_minimum(lst):\n",
    "    if not lst:\n",
    "        return None\n",
    "    \n",
    "    min_val = lst[0]\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] < min_val:\n",
    "            min_val = lst[i]\n",
    "    \n",
    "    return min_val\n",
    "\n",
    "# Test cases\n",
    "print(find_minimum([3, 5, 2, 8, 1]))  # Output: 1\n",
    "print(find_minimum(['apple', 'banana', 'orange', 'pear']))  # Output: 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616781e0-eda2-40ff-b126-d86d933d8879",
   "metadata": {},
   "source": [
    "Here is another example modified from [Jupyter AI documentation](https://github.com/jupyterlab/jupyter-ai?tab=readme-ov-file#the-ai-magic-command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5b9fed7-c957-49f8-a07c-024288552e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\frac{\\partial C}{\\partial t} = D \\nabla^2 C - \\nabla \\cdot (\\mathbf{u}C) + R\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate 3d solute transport equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23f0cda0-2948-46e0-9473-3438603cef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 3D solute transport equation in compact form is given as:\n",
       "\n",
       "∂C/∂t = D∇²C - ∇·(uC) + R\n",
       "\n",
       "where:\n",
       "- C is the concentration of the solute,\n",
       "- t is time,\n",
       "- D is the diffusion coefficient,\n",
       "- ∇² is the Laplacian operator,\n",
       "- u is the velocity vector,\n",
       "- R is the reaction term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f md\n",
    "Generate 3d solute transport equation in compact form with explaintation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8d7833b-77aa-42a2-b42a-e10265c0646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "$$\n",
       "\\frac{\\partial C}{\\partial t} = D \\nabla^2 C - \\nabla \\cdot (\\mathbf{u}C) + R\n",
       "$$\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f markdown\n",
    "Markdown code for 3d solute transport equation in LaTeX surrounded by `$$`. Do not include explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3fce3-4e86-4186-84a1-e70b07ae6627",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{{\\partial c}}{{\\partial t}} = D \\nabla^2 c - \\nabla \\cdot (\\mathbf{v}c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c309b7e-04e8-4614-9f8f-f23c8e5c16c6",
   "metadata": {},
   "source": [
    "#### 2.9.3 The error command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a48be-2f4c-4a58-a6fc-b4c173816cbf",
   "metadata": {},
   "source": [
    "The `error` command explains the most recent error. For usage:\n",
    "```\n",
    "%ai error MODEL_ID\n",
    "```\n",
    "Run the code below, and use error command to understand error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5b4b26c-c26a-42bf-83a8-7143262fc0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a= 1\n",
    "# b= \"2\"\n",
    "# c= 1+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "562ae69e-cd49-45b1-bcb3-c2a31ba85c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "The error is caused because Python does not support addition operation between an integer and a string data type. In this case, the variable 'b' is a string containing the character \"2\", and you are trying to add it to the integer 1. This mismatch in data types leads to the TypeError shown in the error message.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai error chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eafba5-b8a8-4711-b696-b22feb0f5172",
   "metadata": {},
   "source": [
    "To address and rectify this error, you can utilize the list variable `Err[]` or `In[]` as illustrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "876e188a-1774-4af0-ba2e-cd2e69ddeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%ai chatgpt -f code\n",
    "# Fix {Err[19]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b34a89a-48e9-4045-a92d-384f1161b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = str(a) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc2721c1-a361-45e6-8be4-aa9ff44796f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= 1\n",
    "# b= \"2\"\n",
    "# c= str(a) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1edaa22d-df79-4438-82ab-aaec37fd699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = str(a) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303aff3-a37c-462d-8845-085f5fb80b2c",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Note</p>\n",
    "Using list variable Err[] is not advisable for codes with intricate formatting, as it may not yield the desired outcome.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8e5f8-f2b6-4e7e-9a75-ae57cc9d084d",
   "metadata": {},
   "source": [
    "### 2.10 Code Interaction with list variables (*not recommended*)\n",
    "- Pros: Enables working solely within the notebook without the need for a Chat UI interface\n",
    "- Cons: Limited functionality for codes with complex formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e364964-03e0-4298-9a34-5b04eb36793d",
   "metadata": {},
   "source": [
    "Jupyter AI can assist you in interacting with code or markdown cells using Python expressions like `{}`. You can use the special list variables `In[n]`,`Out[n]`, or `Err[n]`: \n",
    "- `{In[n]}`: Retrieves the input \n",
    "- `{Out[n]}`: Retrieves the output  \n",
    "- `{Err[n]}`: Retrives the error \n",
    "\n",
    "of a specific cell where `n` is sequential number that Jupyter notebook  assign to each cell based on execution order in the notebook. This is the number on the left hand side of the cell. For instance, `{In[1]}` would retrieve the input of cell  [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73026210-514f-4694-ab75-f4df493c542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {In[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58328b49-0867-4192-949f-d21685ffaebf",
   "metadata": {},
   "source": [
    "Now you can use these list variables to interact with your Jupyter notebook. \n",
    " \n",
    "Taking the minimum function code above as an example, ask Jupyter AI to:\n",
    "```\n",
    "improve the code and run it for three test cases a list, dictionary, and tuple\n",
    "```\n",
    "Call the `%%ai chatgpt -f code` to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34b4fe09-d508-46fa-8fc9-43251da2d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%ai chatgpt -f code\n",
    "# improve the code below and run it for three test cases a list, dictionary, and tuple:\n",
    "# {In[15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ad7bf93-266c-42a8-b8eb-c930e4a9ecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "apple\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "def find_minimum(iterable):\n",
    "    if not iterable:\n",
    "        return None\n",
    "\n",
    "    min_val = iterable[0]\n",
    "    for val in iterable:\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "\n",
    "    return min_val\n",
    "\n",
    "# Test cases\n",
    "print(find_minimum([3, 5, 2, 8, 1]))  # Output: 1\n",
    "print(find_minimum(['apple', 'banana', 'orange', 'pear']))  # Output: 'apple'\n",
    "print(find_minimum((45, 21, 37, 58, 12)))  # Output: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab9b4bb1-49a1-4669-8a8e-face220c5e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "def find_minimum(data):\n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    if isinstance(data, list) or isinstance(data, tuple):\n",
    "        min_val = data[0]\n",
    "        for i in range(1, len(data)):\n",
    "            if data[i] < min_val:\n",
    "                min_val = data[i]\n",
    "        return min_val\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        return min(data.values())\n",
    "\n",
    "# Test cases\n",
    "print(find_minimum([3, 5, 2, 8, 1]))  # Output: 1\n",
    "print(find_minimum({'a': 5, 'b': 3, 'c': 7, 'd': 1}))  # Output: 1\n",
    "print(find_minimum(('apple', 'banana', 'orange', 'pear')))  # Output: 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931435be-2681-4025-836a-2e3a03cfb3bf",
   "metadata": {},
   "source": [
    "### 2.11 Code Interaction with Chat UI (*recommended*)\n",
    "- Pros: Provides the capability to execute various tasks, as demonstrated below\n",
    "- Cons: Requires more typing to customize the output as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c607b-6958-4bca-a57f-51a127bd1b18",
   "metadata": {},
   "source": [
    "With Chat UI you can ask your LM to perform many coding tasks.\n",
    "- **Complete:** LM provides code completion as suggested by developer\n",
    "- **Debug:** LM  debugs an error message in your code\n",
    "- **Expalain:** LM provides explanations, documentation, and insights about the code or part of the code \n",
    "- **Translate:** LM translates codes between different programming languages or paradigms like converting flowchart symbols to a code to a figure \n",
    "- **Review:** LM reviews and suggests refactoring improvements to existing code such as optimizing performance, improving readability, or adhering to best practices\n",
    "- **Format:** LM automatically adds comments, docstrings, formatting to code cell, and formatting to markdown cell\n",
    "- **Troubleshoot:** LM troubleshoots errors when installing a new package \n",
    "- **Spellcheck:** LM corrects your language errors\n",
    "- **Improve:** LM can improve your content\n",
    "- **Chat:** LM answers your questions and provide information\n",
    "- **And much more**: LM can perform many other tasks in your Jupyter notebook\n",
    "\n",
    "The idea is simple. You have a chat user-interface that allows you to\n",
    "- ask questions\n",
    "- include selection from a code or markdown cell\n",
    "- replace selection from a code or markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae41c7-2287-410f-8e7b-3a4991ad9f8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "You can try out this code completion example:\n",
    "```python\n",
    "# Generate a Pandas DataFrame with daily data from 2020-01-01 to 2023-12-31 in Fort Myers, Florida \n",
    "#columns: \n",
    "#(1) 'TMIN' that is the minimum temperature, \n",
    "#(2) 'TMAX' that is the maximum temperature, \n",
    "#(3) 'PRCP' that is precipitation in inches, \n",
    "#(4) 'AWDS' that is the average wind speed in miles per hour, \n",
    "# (5) 'STATION' which has two stations, 'Field Airport' and 'SWF Airport'. \n",
    "\n",
    "# The index is the date\n",
    "\n",
    "# Display the DataFrame in JupyterLab\n",
    "\n",
    "# Pandas operation to find rows of the days that has the maximum precipitation \n",
    "# in the study period  for each of the two stations for each year\n",
    "\n",
    "# Display the DataFrame in JupyterLab\n",
    "```\n",
    "Try this below with using the Chat UI by copying and pasting the above incomplete code below and asking your LM to complete this code and return code only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30691c96-6c55-43e1-bb13-7270d3fc210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>AWDS</th>\n",
       "      <th>STATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>59</td>\n",
       "      <td>89</td>\n",
       "      <td>1.818940</td>\n",
       "      <td>11</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>0.345554</td>\n",
       "      <td>5</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>0.738174</td>\n",
       "      <td>13</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>59</td>\n",
       "      <td>95</td>\n",
       "      <td>1.835586</td>\n",
       "      <td>5</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>73</td>\n",
       "      <td>92</td>\n",
       "      <td>0.605059</td>\n",
       "      <td>14</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>0.905490</td>\n",
       "      <td>7</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>61</td>\n",
       "      <td>91</td>\n",
       "      <td>0.712882</td>\n",
       "      <td>11</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>1.013225</td>\n",
       "      <td>12</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>82</td>\n",
       "      <td>76</td>\n",
       "      <td>1.182195</td>\n",
       "      <td>6</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>59</td>\n",
       "      <td>73</td>\n",
       "      <td>0.828091</td>\n",
       "      <td>7</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1461 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TMIN  TMAX      PRCP  AWDS        STATION\n",
       "2020-01-01    59    89  1.818940    11    SWF Airport\n",
       "2020-01-02    74    73  0.345554     5  Field Airport\n",
       "2020-01-03    74    74  0.738174    13  Field Airport\n",
       "2020-01-04    59    95  1.835586     5    SWF Airport\n",
       "2020-01-05    73    92  0.605059    14  Field Airport\n",
       "...          ...   ...       ...   ...            ...\n",
       "2023-12-27    86    82  0.905490     7    SWF Airport\n",
       "2023-12-28    61    91  0.712882    11  Field Airport\n",
       "2023-12-29    73    86  1.013225    12    SWF Airport\n",
       "2023-12-30    82    76  1.182195     6  Field Airport\n",
       "2023-12-31    59    73  0.828091     7  Field Airport\n",
       "\n",
       "[1461 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate data\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31')\n",
    "stations = ['Field Airport', 'SWF Airport']\n",
    "data = {\n",
    "    'TMIN': np.random.randint(50, 90, len(dates)),\n",
    "    'TMAX': np.random.randint(70, 100, len(dates)),\n",
    "    'PRCP': np.random.uniform(0, 2, len(dates)),\n",
    "    'AWDS': np.random.randint(5, 15, len(dates)),\n",
    "    'STATION': np.random.choice(stations, len(dates))\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b286a5b-da32-4131-9203-8fe49bafdfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>AWDS</th>\n",
       "      <th>STATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-29</th>\n",
       "      <td>63</td>\n",
       "      <td>99</td>\n",
       "      <td>1.984615</td>\n",
       "      <td>7</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>1.989527</td>\n",
       "      <td>6</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-29</th>\n",
       "      <td>85</td>\n",
       "      <td>84</td>\n",
       "      <td>1.998213</td>\n",
       "      <td>5</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-28</th>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>1.993168</td>\n",
       "      <td>11</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-29</th>\n",
       "      <td>56</td>\n",
       "      <td>85</td>\n",
       "      <td>1.997925</td>\n",
       "      <td>7</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-16</th>\n",
       "      <td>52</td>\n",
       "      <td>86</td>\n",
       "      <td>1.969981</td>\n",
       "      <td>6</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-13</th>\n",
       "      <td>69</td>\n",
       "      <td>72</td>\n",
       "      <td>1.988529</td>\n",
       "      <td>7</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-16</th>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>1.996784</td>\n",
       "      <td>6</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TMIN  TMAX      PRCP  AWDS        STATION\n",
       "2020-01-29    63    99  1.984615     7  Field Airport\n",
       "2020-04-09    68    86  1.989527     6    SWF Airport\n",
       "2021-09-29    85    84  1.998213     5  Field Airport\n",
       "2021-04-28    77    82  1.993168    11    SWF Airport\n",
       "2022-10-29    56    85  1.997925     7  Field Airport\n",
       "2022-11-16    52    86  1.969981     6    SWF Airport\n",
       "2023-01-13    69    72  1.988529     7  Field Airport\n",
       "2023-09-16    82    79  1.996784     6    SWF Airport"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find rows with max precipitation for each station for each year\n",
    "max_indices = df.groupby([df.index.year, 'STATION'])['PRCP'].idxmax()\n",
    "result = df.loc[max_indices]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aded254-8dec-4424-aa6a-b9282f35e53e",
   "metadata": {},
   "source": [
    "## 3. Class exercise\n",
    "\n",
    "Complete this exercise by utilizing: \n",
    "- Jupyter AI,\n",
    "- another AI chat assistant,\n",
    "- or any Language Model (LM) of your preference directly without an AI chat assistant.\n",
    "\n",
    "The exercise aims to teach the utilization of Language Models (LMs) for coding assistance and emphasizes the significance of prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5075db-6907-4c84-9ac4-2b7616f37e0d",
   "metadata": {},
   "source": [
    "### 3.1 Problem statement\n",
    "An student asked: For a Pandas DataFrame, how to display the rows of the days with the maximum precipitation for each weather station in each year in our study period and area? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b46f5-14a9-4e22-9b13-3df8f2dd8113",
   "metadata": {},
   "source": [
    "### 3.2 Prompt engineering and code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12049d-0350-4e47-a529-41df55053585",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Note</p>\n",
    "Prompt engineering involves crafting and refining the language or structure of prompts to improve the performance of a language model in generating accurate and relevant responses. Mastering prompt engineering enables effective utilization of LMs across tasks from problem-solving to creative writing. Learn more with Real Python's tutorial  <a href=\"https://realpython.com/practical-prompt-engineering/\">Prompt Engineering: A Practical Example</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddeba1-830f-4523-9620-f5c787b2112f",
   "metadata": {},
   "source": [
    "Here is one prompt that we can start with and refine later as needed: \n",
    "```\n",
    "Generate a Pandas DataFrame with daily data from 2020-01-01 to 2023-12-31 in Fort Myers, Florida with the following columns:\n",
    "(1) 'TMIN' that is the minimum temperature,\n",
    "(2) 'TMAX' that is the maximum temperature,\n",
    "(3) 'PRCP' that is precipitation in inches,\n",
    "(4) 'AWDS' that is the average wind speed in miles per hour,\n",
    "and (5) 'STATION' which has two stations, 'Field Airport' and 'SWF Airport'.\n",
    "\n",
    "The index is the date.\n",
    "\n",
    "Display the DataFrame to screen.\n",
    "\n",
    "Find and display the rows of the days that has the maximum precipitation in the study period for each of the two stations for each year.\n",
    "```\n",
    "Let use see if our select LM can do this. You can use an AI code assistant such as Jupter AI or directly use any LM of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "363eeafb-cd23-4a81-a95d-5a21eac436eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f code\n",
    "Generate a Pandas DataFrame with daily data from 2020-01-01 to 2023-12-31 in Fort Myers, Florida with the following columns:\n",
    "(1) 'TMIN' that is the minimum temperature,\n",
    "(2) 'TMAX' that is the maximum temperature,\n",
    "(3) 'PRCP' that is precipitation in inches,\n",
    "(4) 'AWDS' that is the average wind speed in miles per hour,\n",
    "and (5) 'STATION' which has two stations, 'Field Airport' and 'SWF Airport'.\n",
    "\n",
    "The index is the date.\n",
    "\n",
    "Display the DataFrame to screen.\n",
    "\n",
    "Find and display the rows of the days that has the maximum precipitation in the study period for each of the two stations for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea03a2-9713-4fd8-a0f6-64580c469cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'TMIN': np.random.randint(50, 90, len(dates)),\n",
    "    'TMAX': np.random.randint(70, 100, len(dates)),\n",
    "    'PRCP': np.random.randint(0, 5, len(dates)) + np.random.rand(len(dates)),\n",
    "    'AWDS': np.random.randint(5, 20, len(dates)) + np.random.rand(len(dates)),\n",
    "    'STATION': np.random.choice(['Field Airport', 'SWF Airport'], len(dates))\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "\n",
    "# Rows with maximum precipitation for each station in each year\n",
    "max_precip_by_year = df.groupby([df.index.year, 'STATION'])['PRCP'].idxmax()\n",
    "df.loc[max_precip_by_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1718c63-953b-42d2-a144-e9394e317762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TMIN  TMAX      PRCP       AWDS        STATION\n",
      "2020-01-01    63    93  0.412416  10.829396  Field Airport\n",
      "2020-01-02    57    82  1.113892  11.162640  Field Airport\n",
      "2020-01-03    50    94  1.601719   5.448287    SWF Airport\n",
      "2020-01-04    62    92  0.567067  10.960894    SWF Airport\n",
      "2020-01-05    55    98  0.978845  11.151728    SWF Airport\n",
      "...          ...   ...       ...        ...            ...\n",
      "2023-12-27    55    84  1.631453   6.278442    SWF Airport\n",
      "2023-12-28    74    99  0.091821  14.832274    SWF Airport\n",
      "2023-12-29    56    94  0.868538  11.198596    SWF Airport\n",
      "2023-12-30    65    81  1.921286  14.439996    SWF Airport\n",
      "2023-12-31    55    85  1.917850   9.770754  Field Airport\n",
      "\n",
      "[1461 rows x 5 columns]\n",
      "            TMIN  TMAX      PRCP       AWDS        STATION\n",
      "2020-11-03    66    97  1.996768  13.634396  Field Airport\n",
      "2020-11-17    62    96  1.990162  11.261898    SWF Airport\n",
      "2021-10-26    68    84  1.997440   7.263059  Field Airport\n",
      "2021-09-26    68    81  1.954115   9.469243    SWF Airport\n",
      "2022-01-14    57    86  1.998555  14.665895  Field Airport\n",
      "2022-08-29    63    86  1.991698  14.148549    SWF Airport\n",
      "2023-12-12    54    96  1.965353  14.050801  Field Airport\n",
      "2023-05-17    68    82  1.976660  11.127668    SWF Airport\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "data = {\n",
    "    'TMIN': np.random.randint(50, 80, len(dates)),\n",
    "    'TMAX': np.random.randint(80, 100, len(dates)),\n",
    "    'PRCP': np.random.uniform(0, 2, len(dates)),\n",
    "    'AWDS': np.random.uniform(5, 15, len(dates)),\n",
    "    'STATION': np.random.choice(['Field Airport', 'SWF Airport'], len(dates))\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "\n",
    "print(df)\n",
    "\n",
    "max_precipitation = df.groupby([df.index.year, 'STATION'])['PRCP'].idxmax()\n",
    "print(df.loc[max_precipitation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "358deeaf-e6af-47df-9fe3-ede7e6078e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TMIN  TMAX      PRCP  AWDS        STATION\n",
      "2020-07-01    53    96  1.998789     7  Field Airport\n",
      "            TMIN  TMAX      PRCP  AWDS      STATION\n",
      "2020-02-19    68    92  1.991963     6  SWF Airport\n",
      "            TMIN  TMAX      PRCP  AWDS        STATION\n",
      "2021-09-16    71    73  1.989236     6  Field Airport\n",
      "            TMIN  TMAX      PRCP  AWDS      STATION\n",
      "2021-01-04    78    94  1.991905     8  SWF Airport\n",
      "            TMIN  TMAX      PRCP  AWDS        STATION\n",
      "2022-02-26    56    81  1.996354     7  Field Airport\n",
      "            TMIN  TMAX      PRCP  AWDS      STATION\n",
      "2022-04-14    59    85  1.998168    12  SWF Airport\n",
      "            TMIN  TMAX      PRCP  AWDS        STATION\n",
      "2023-04-25    86    99  1.993338    13  Field Airport\n",
      "            TMIN  TMAX      PRCP  AWDS      STATION\n",
      "2023-09-06    51    90  1.961782     7  SWF Airport\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'TMIN': np.random.randint(50, 90, len(dates)),\n",
    "    'TMAX': np.random.randint(70, 100, len(dates)),\n",
    "    'PRCP': np.random.uniform(0, 2, len(dates)),\n",
    "    'AWDS': np.random.randint(5, 15, len(dates)),\n",
    "    'STATION': np.random.choice(['Field Airport', 'SWF Airport'], len(dates))\n",
    "}\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "\n",
    "# Find and display rows with maximum precipitation for each station for each year\n",
    "for year in range(2020, 2024):\n",
    "    for station in ['Field Airport', 'SWF Airport']:\n",
    "        max_precipitation = df[(df.index.year == year) & (df['STATION'] == station)]['PRCP'].max()\n",
    "        max_precipitation_rows = df[(df.index.year == year) & (df['STATION'] == station) & (df['PRCP'] == max_precipitation)]\n",
    "        print(max_precipitation_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc941d-964d-4c17-8b76-9ac4f8aa2e50",
   "metadata": {},
   "source": [
    "This is a promising beginning.\n",
    "\n",
    "We need to verify the output and not just rely on everything that our LM is providing. The code snippet above exhibits a few issues:\n",
    "1. Our current LM setup does not have access to datasets.\n",
    "2. It employs a for loop instead of utilizing Pandas operations.\n",
    "3. It utilizes `print` instead of the `display` function, which presents data in a visually appealing tabular format for Jupyter notebooks.\n",
    "\n",
    "To tackle the first issue, we can instruct our LM to access a specific data file online or on our machine, instead of generating random data. \n",
    "\n",
    "Let us now address the second and third problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad171586-2bd0-4ae9-85cf-d50c19c7a92e",
   "metadata": {},
   "source": [
    "### 3.3 Code improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c0a34-eb32-47ac-b085-368c8c683a90",
   "metadata": {},
   "source": [
    "Ask your LM to use Pandas operations instead of for loop, and to display results in JupyterLab that is to use `display` instead of `print` function.\n",
    "\n",
    "Here is what the LM sugguested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f452a4a-74b9-4ede-953d-aded8363cce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>AWDS</th>\n",
       "      <th>STATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>57</td>\n",
       "      <td>79</td>\n",
       "      <td>1.992814</td>\n",
       "      <td>14</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-15</th>\n",
       "      <td>51</td>\n",
       "      <td>91</td>\n",
       "      <td>1.998155</td>\n",
       "      <td>11</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>55</td>\n",
       "      <td>98</td>\n",
       "      <td>1.999462</td>\n",
       "      <td>13</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-24</th>\n",
       "      <td>66</td>\n",
       "      <td>78</td>\n",
       "      <td>1.986984</td>\n",
       "      <td>14</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-05</th>\n",
       "      <td>65</td>\n",
       "      <td>77</td>\n",
       "      <td>1.999682</td>\n",
       "      <td>13</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-18</th>\n",
       "      <td>51</td>\n",
       "      <td>73</td>\n",
       "      <td>1.994036</td>\n",
       "      <td>11</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-11</th>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>1.999237</td>\n",
       "      <td>7</td>\n",
       "      <td>Field Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-04</th>\n",
       "      <td>72</td>\n",
       "      <td>85</td>\n",
       "      <td>1.993916</td>\n",
       "      <td>13</td>\n",
       "      <td>SWF Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TMIN  TMAX      PRCP  AWDS        STATION\n",
       "2020-09-30    57    79  1.992814    14  Field Airport\n",
       "2020-08-15    51    91  1.998155    11    SWF Airport\n",
       "2021-03-26    55    98  1.999462    13  Field Airport\n",
       "2021-01-24    66    78  1.986984    14    SWF Airport\n",
       "2022-06-05    65    77  1.999682    13  Field Airport\n",
       "2022-11-18    51    73  1.994036    11    SWF Airport\n",
       "2023-11-11    86    70  1.999237     7  Field Airport\n",
       "2023-11-04    72    85  1.993916    13    SWF Airport"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create date range\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'TMIN': np.random.randint(50, 90, len(dates)),\n",
    "    'TMAX': np.random.randint(70, 100, len(dates)),\n",
    "    'PRCP': np.random.uniform(0, 2, len(dates)),\n",
    "    'AWDS': np.random.randint(5, 15, len(dates)),\n",
    "    'STATION': np.random.choice(['Field Airport', 'SWF Airport'], len(dates))\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=dates)\n",
    "\n",
    "# Find and display rows with maximum precipitation for each year and station using Pandas operations\n",
    "max_precipitation_rows = df.loc[df.groupby([df.index.year, 'STATION'])['PRCP'].idxmax()]\n",
    "display(max_precipitation_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a3bbc-b7e3-4dda-b7da-ef730f2272ea",
   "metadata": {},
   "source": [
    "### 3.4 Handling challanging problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f7cb3-d4b9-411c-8cae-c588147a94c1",
   "metadata": {},
   "source": [
    "The above example demonstrates that our LM, which is ChatGPT 3.5 Turbo, is able to solve this relatively straightforward problem. However, a more challenging problem might require additional strategies to handle effectively. \n",
    "  \n",
    "When faced with a more complex problem, several approaches can be beneficial. I asked my LM to complete this section for me. The LM suggests the first four points. I added the heading of point 5 and asked my LM to complete it for me: \n",
    "1. **Break Down the Problem**: If the problem is complex, breaking it down into smaller, more manageable sub-problems can help. Providing step-by-step instructions or dividing the problem into sequential tasks can guide the model in tackling each part systematically.\n",
    "2. **Provide Context and Examples**: Offering context, examples, or related information can assist the model in understanding the problem better. Clear descriptions, relevant data samples, or background information can enhance the model's comprehension and problem-solving capabilities.\n",
    "3. **Ask Specific Questions**: Instead of presenting a broad or vague problem statement, asking specific questions or providing precise requirements can help direct the model's attention to the key aspects of the problem.\n",
    "4. **Iterative Approach**: In cases where the problem is intricate, an iterative approach may be beneficial. Engaging in a dialogue with the model, providing feedback on its responses, and refining the problem statement based on initial outputs can lead to a more targeted and accurate solution.\n",
    "5. **Consider Advanced Language Models**: By leveraging a robust language model, you can potentially achieve more accurate results, handle more complex patterns, and tackle a wider range of tasks with greater efficiency.\n",
    "\n",
    "Now let us try using different version of ChatGPT 4 in [Chatbot Arena](https://chat.lmsys.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939d59f-cbf9-4523-b94d-c9d70cce43f5",
   "metadata": {},
   "source": [
    "## 4. Other useful tools \n",
    "[Chatbot Arena](https://chat.lmsys.org) is an open-source research project developed to an open crowdsourced platform to evaluate LMs.\n",
    "\n",
    "Let us try to use [Chatbot Arena](https://chat.lmsys.org). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa85a3-f502-4de2-b23c-f0d91f68a423",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "Here are the key points to consider:\n",
    "\n",
    "- **Pros of Language Models**: These models offer the potential to improve various aspects of the coding process, spanning from initial development to code optimization.\n",
    "- **Cons of Language Models**: Drawbacks include creating a dependency on AI for coding tasks can hinder personal skill development; may result in code plagiarism and lack of originality; no guarantee of error-free code without human review; and the inability to provide creative solutions that require human insight.\n",
    "- **Effective Prompts**: Step-by-step, detailed, clear, precise, and contextually relevant prompts can proficiently guide language models towards precise and targeted responses.\n",
    "- **AI Code Assistants**: Tools like Jupyter AI can boost your Python learning and productivity by aiding in coding tasks directly within your integrated development environment (IDE), such as JupyterLab.\n",
    "\n",
    "To sum up, AI assistanance is not here to replace the work that you do, but to help you. Try to balance the benefits of AI assistance with the need for personal skill development and critical thinking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
