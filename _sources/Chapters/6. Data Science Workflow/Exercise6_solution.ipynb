{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - AQI Data Preparation\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/aselshall/edsbook/refs/heads/master/figures/workflow3.png)\n",
    "This lesson was developed with assistance from GPT-4-1106-preview, claude-2.1, and GPT-3.5-turbo.\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/aselshall/eds/HEAD)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem definition\n",
    "\n",
    "What is the impact of COVID19 lockdown orders on air quality in Miami?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data requirements\n",
    "\n",
    "We need to collect daily air quality data for parameters such as PM2.5, PM10, NO2, SO2, and CO in csv format from 2019 to 2021. That is one year before and one year after the lockdown orders of COVID 19 in April 2020. [EPA Air Data](https://www.epa.gov/outdoor-air-quality-data)  contains air quality data collected at outdoor monitors across the US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation \n",
    "\n",
    "- Data discovery and profiling: Exploring the content, quality, and structure of the dataset\n",
    "- Data collection: Gathering raw data from various sources for analysis\n",
    "- Data cleaning: Standardizing inconsistent data formats, correcting errors, and handling missing values\n",
    "- Data structuring: Organizing data for efficient storage and retrieval\n",
    "- Data enrichment: Enhancing dataset with additional information from external sources to improve analysis \n",
    "- Data transformation: Optimizing the content and structure of data for further analysis\n",
    "- Data validation: Ensuring the accuracy, completeness, and reliability of the data\n",
    "- Data integration: Combining data from multiple sources to create a unified dataset for analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Data discovery and profiling\n",
    "\n",
    "In this step we are exploring the content, quality, and structure of the dataset. From this step we can learn the following.\n",
    "- [EPA Air Data](https://www.epa.gov/outdoor-air-quality-data)  contains air quality data collected at outdoor monitors across the US.\n",
    "- For this exercise we want to use [Pre-Generated Data Files - Daily Data](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily).\n",
    "- For each year one data quality parameter (e.g., NO2, SO2, and CO) is recorded in a csv file.\n",
    "- The dataset have 16 air quality parameters.\n",
    "- For three years, there are 48 csv files and each file is about 60 MB. That is approximately 3 GB of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data collection\n",
    "\n",
    "In this step we want to gather raw data for analysis The breakdown of this step is as follows.\n",
    "- We need to collect data for 16 air quality parameters for three years from [Pre-Generated Data Files - Daily Data](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily) that is 48 files\n",
    "- Each file is in csv format and zipped\n",
    "- We can use Python tools for making HTTP requests such as `request` package\n",
    "- We can use Python tools for unzipping files such as `zipfile` package\n",
    "- Each file is large than 60MB, so the download and processing of raw data can be long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data cleaning\n",
    "\n",
    "This step involves standardizing inconsistent data formats, correcting errors, and handling missing values. For this step we will do the following:\n",
    "- For this EPA data, cleaning is minimal\n",
    "- We will record the files that has no records for our study area and period\n",
    "- We do not need to do anything about missing data at this stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data structuring\n",
    "\n",
    "This steps involves organizing and formating the data in a way that is optimal for storage and retrieval. For this step we need to do the following tasks. \n",
    "- To reduce storage space, we will filter out rows outside our study region\n",
    "- We will only keep relevant columns for our record and analysis that are: 'County Code', 'Parameter Name', 'Date Local', 'Units of Measure', 'Arithmetic Mean', 'CBSA Name'.\n",
    "- This will reduce the storgage space per file from from ~60 MB to ~0.4 MB.\n",
    "\n",
    "Note: A core-based statistical area (CBSA) is a geographic region of the U.S.as defined by the Office of Management and Budget.A CBSA is \"one or more adjacent counties or county equivalents that have at least one urban core area of at least 10,000 population, plus adjacent territory that has a high degree of social and economic integration with the core as measured by commuting ties\" (Wikipedia). \n",
    "\n",
    "After conducting an initial data exploration and profiling, we can formulate a prompt  our language model to assist with data collection, cleaning, and structuring. Here is a prompot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python code with rich annotations:**  \n",
    "This code  collects air quality data in a `region` for one year before and one year after the lockdown order on April 1, 2020. \n",
    "\n",
    "These are the FileIDs of the air quality data that we want to download:\n",
    "| Category                         | Parameter                     | FileID | Unit | Records |\n",
    "|----------------------------------|-------------------------------|--------|------|---|\n",
    "| Criteria Gases                   | Ozone                         | 44201  | na  | 0 |\n",
    "| Criteria Gases                   | SO2                           | 42401  | na | 0 |\n",
    "| Criteria Gases                   | CO                            | 42101  | na | 0 |\n",
    "| Criteria Gases                   | NO2                           | 42602  | na | 0 |\n",
    "| Particulates                     | PM2.5 FRM/FEM Mass            | 88101  | na | 0 |\n",
    "| Particulates                     | PM2.5 non FRM/FEM Mass        | 88502  | na | 0 |\n",
    "| Particulates                     | PM10 Mass                     | 81102  | na | 0 |\n",
    "| Particulates                     | PMc Mass                      | 86101  | na | 0 |\n",
    "| Meteorological                   | Winds                         | WIND   | na | 0 |\n",
    "| Meteorological                   | Temperature                   | TEMP   | na | 0 |\n",
    "| Meteorological                   | Barometric Pressure           | PRESS  | na | 0 |\n",
    "| Meteorological                   | RH and Dewpoint               | RH_DP  | na | 0 |\n",
    "| Toxics, Precursors, and Lead     | HAPs                          | HAPS   | na | 0 |\n",
    "| Toxics, Precursors, and Lead     | VOCs                          | VOCS   | na | 0 |\n",
    "| Toxics, Precursors, and Lead     | NONOxNOy                      | NONOxNOy| na | 0 |\n",
    "| Toxics, Precursors, and Lead     | Lead                          | LEAD   | na | 0 |\n",
    "\n",
    "The daily data file is named 'daily_FileID_Year.zip'.  \n",
    "For example, 'daily_44201_2023.zip' has Ozone data for 2023\n",
    "  \n",
    "The file address is: https://aqs.epa.gov/aqsweb/airdata/daily_44201_2023.zip  \n",
    "For example,\n",
    "```python\n",
    "filename = 'daily_44201_2023.zip' \n",
    "base_url = 'https://aqs.epa.gov/aqsweb/airdata/\n",
    "download_from = base_url + filename\n",
    "```\n",
    "will be the address of the Ozone data for 2023.\n",
    "   \n",
    "Each zip file contains a csv file. \n",
    "For example, 'daily_44201_2023.zip' contains 'daily_44201_2023.csv'\n",
    "\n",
    "Addtional information\n",
    "```python\n",
    "#These files will be downloaded to:\n",
    "save_to = 'Data\\L2_Structured\"\n",
    "\n",
    "# Region of interest\n",
    "region = 'Miami'\n",
    "\n",
    "# CBSA Name of the region of interest \n",
    "CBSA_Name_Value = 'Miami-Fort Lauderdale-West Palm Beach, FL'\n",
    "```\n",
    "\n",
    "Write a Python code to:\n",
    "1. Use the information in th table above to create `Attributes` list or dictionary for 'Category', 'Parameter', 'FileID', 'Unit', 'Records'\n",
    "2. Define variables `region` and `CBSA_Name_Value`\n",
    "3. Count each download with variable name `download_count`\n",
    "4. Get files for years 2019, 2020, 2021 for all the FileIDs\n",
    "5. Show message about download progress after each fileas as `download_count` : `year` : `parameter` : `filename`\n",
    "6. Unzip each file\n",
    "7. Read csv file with Pandas as `raw_data` DataFrame\n",
    "8. Delete zip file: 'daily_FileID_Year.zip' only if it exists and if file is open, close it. For example, 'daily_44201_2023.zip'\n",
    "9. Delete csv file: 'daily_FileID_Year.csv' only if it exists and if file is open, close it. For example, 'daily_44201_2023.csv'\n",
    "10. In `raw_data`, address this warning: \"Columns (13) have mixed types. Specify dtype option on import or set low_memory=False\".\n",
    "11. In `Attributes`, update 'Unit' with the first value in column 'Units of Measure' in `raw_data`. For example, for fileid = 44201, 'na' will be replaced with 'Parts per million'. If the first value does not exist update 'Unit' in `Attributes` list with 'empy_file'\n",
    "12. In `raw_data`, keep only rows where the value of Column 'CBSA Name' is equal to CBSA_Name_Value\n",
    "13. In `raw_data`, keep only columns with labels: 'County Code', 'Parameter Name', 'Date Local', 'Units of Measure', 'Arithmetic Mean', 'CBSA Name'\n",
    "14. In `raw_data`, convert 'Date Local' column to datetime format, set as index, and sort by index\n",
    "15. In `Attributes`, update 'Records'  with the number of rows in the `raw_data` plus the current value of 'Records'\n",
    "16. Save `raw_data` at `save_to` location with file name 'daily_{FileID}_{year}_{region}.csv2019, and create folder if it does not exists\n",
    "17. At the end from `Attributes` create `df_Attributes` DataFrame \n",
    "18. Save  `df_Attributes` at `save_to` as 'Attributes.cs \n",
    "19. Display `df_Attributes` DataFrame\n",
    "20. Annotate code with detailed and rich annotations describing each task and sub-task inside and outside the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 1: 2019 : Ozone : daily_44201_2019.zip\n",
      "Download 2: 2019 : SO2 : daily_42401_2019.zip\n",
      "Download 3: 2019 : CO : daily_42101_2019.zip\n",
      "Download 4: 2019 : NO2 : daily_42602_2019.zip\n",
      "Download 5: 2019 : PM2.5 FRM/FEM Mass : daily_88101_2019.zip\n",
      "Download 6: 2019 : PM2.5 non FRM/FEM Mass : daily_88502_2019.zip\n",
      "Download 7: 2019 : PM10 Mass : daily_81102_2019.zip\n",
      "Download 8: 2019 : PMc Mass : daily_86101_2019.zip\n",
      "Download 9: 2019 : Winds : daily_WIND_2019.zip\n",
      "Download 10: 2019 : Temperature : daily_TEMP_2019.zip\n",
      "Download 11: 2019 : Barometric Pressure : daily_PRESS_2019.zip\n",
      "Download 12: 2019 : RH and Dewpoint : daily_RH_DP_2019.zip\n",
      "Download 13: 2019 : HAPs : daily_HAPS_2019.zip\n",
      "Download 14: 2019 : VOCs : daily_VOCS_2019.zip\n",
      "Download 15: 2019 : NONOxNOy : daily_NONOxNOy_2019.zip\n",
      "Download 16: 2019 : Lead : daily_LEAD_2019.zip\n",
      "Download 17: 2020 : Ozone : daily_44201_2020.zip\n",
      "Download 18: 2020 : SO2 : daily_42401_2020.zip\n",
      "Download 19: 2020 : CO : daily_42101_2020.zip\n",
      "Download 20: 2020 : NO2 : daily_42602_2020.zip\n",
      "Download 21: 2020 : PM2.5 FRM/FEM Mass : daily_88101_2020.zip\n",
      "Download 22: 2020 : PM2.5 non FRM/FEM Mass : daily_88502_2020.zip\n",
      "Download 23: 2020 : PM10 Mass : daily_81102_2020.zip\n",
      "Download 24: 2020 : PMc Mass : daily_86101_2020.zip\n",
      "Download 25: 2020 : Winds : daily_WIND_2020.zip\n",
      "Download 26: 2020 : Temperature : daily_TEMP_2020.zip\n",
      "Download 27: 2020 : Barometric Pressure : daily_PRESS_2020.zip\n",
      "Download 28: 2020 : RH and Dewpoint : daily_RH_DP_2020.zip\n",
      "Download 29: 2020 : HAPs : daily_HAPS_2020.zip\n",
      "Download 30: 2020 : VOCs : daily_VOCS_2020.zip\n",
      "Download 31: 2020 : NONOxNOy : daily_NONOxNOy_2020.zip\n",
      "Download 32: 2020 : Lead : daily_LEAD_2020.zip\n",
      "Download 33: 2021 : Ozone : daily_44201_2021.zip\n",
      "Download 34: 2021 : SO2 : daily_42401_2021.zip\n",
      "Download 35: 2021 : CO : daily_42101_2021.zip\n",
      "Download 36: 2021 : NO2 : daily_42602_2021.zip\n",
      "Download 37: 2021 : PM2.5 FRM/FEM Mass : daily_88101_2021.zip\n",
      "Download 38: 2021 : PM2.5 non FRM/FEM Mass : daily_88502_2021.zip\n",
      "Download 39: 2021 : PM10 Mass : daily_81102_2021.zip\n",
      "Download 40: 2021 : PMc Mass : daily_86101_2021.zip\n",
      "Download 41: 2021 : Winds : daily_WIND_2021.zip\n",
      "Download 42: 2021 : Temperature : daily_TEMP_2021.zip\n",
      "Download 43: 2021 : Barometric Pressure : daily_PRESS_2021.zip\n",
      "Download 44: 2021 : RH and Dewpoint : daily_RH_DP_2021.zip\n",
      "Download 45: 2021 : HAPs : daily_HAPS_2021.zip\n",
      "Download 46: 2021 : VOCs : daily_VOCS_2021.zip\n",
      "Download 47: 2021 : NONOxNOy : daily_NONOxNOy_2021.zip\n",
      "Download 48: 2021 : Lead : daily_LEAD_2021.zip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>FileID</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Criteria Gases</td>\n",
       "      <td>Ozone</td>\n",
       "      <td>44201</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>8420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Criteria Gases</td>\n",
       "      <td>SO2</td>\n",
       "      <td>42401</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>4253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Criteria Gases</td>\n",
       "      <td>CO</td>\n",
       "      <td>42101</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>6256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Criteria Gases</td>\n",
       "      <td>NO2</td>\n",
       "      <td>42602</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>5056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Particulates</td>\n",
       "      <td>PM2.5 FRM/FEM Mass</td>\n",
       "      <td>88101</td>\n",
       "      <td>Micrograms/cubic meter (LC)</td>\n",
       "      <td>14924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Particulates</td>\n",
       "      <td>PM2.5 non FRM/FEM Mass</td>\n",
       "      <td>88502</td>\n",
       "      <td>Micrograms/cubic meter (LC)</td>\n",
       "      <td>7987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Particulates</td>\n",
       "      <td>PM10 Mass</td>\n",
       "      <td>81102</td>\n",
       "      <td>Micrograms/cubic meter (25 C)</td>\n",
       "      <td>4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Particulates</td>\n",
       "      <td>PMc Mass</td>\n",
       "      <td>86101</td>\n",
       "      <td>Micrograms/cubic meter (LC)</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Meteorological</td>\n",
       "      <td>Winds</td>\n",
       "      <td>WIND</td>\n",
       "      <td>Knots</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Meteorological</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>TEMP</td>\n",
       "      <td>Degrees Fahrenheit</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Meteorological</td>\n",
       "      <td>Barometric Pressure</td>\n",
       "      <td>PRESS</td>\n",
       "      <td>Millibars</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Meteorological</td>\n",
       "      <td>RH and Dewpoint</td>\n",
       "      <td>RH_DP</td>\n",
       "      <td>Percent relative humidity</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Toxics, Precursors, and Lead</td>\n",
       "      <td>HAPs</td>\n",
       "      <td>HAPS</td>\n",
       "      <td>Micrograms/cubic meter (LC)</td>\n",
       "      <td>6536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Toxics, Precursors, and Lead</td>\n",
       "      <td>VOCs</td>\n",
       "      <td>VOCS</td>\n",
       "      <td>Parts per billion Carbon</td>\n",
       "      <td>7433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Toxics, Precursors, and Lead</td>\n",
       "      <td>NONOxNOy</td>\n",
       "      <td>NONOxNOy</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>12028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Toxics, Precursors, and Lead</td>\n",
       "      <td>Lead</td>\n",
       "      <td>LEAD</td>\n",
       "      <td>Micrograms/cubic meter (LC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Category               Parameter    FileID  \\\n",
       "0                 Criteria Gases                   Ozone     44201   \n",
       "1                 Criteria Gases                     SO2     42401   \n",
       "2                 Criteria Gases                      CO     42101   \n",
       "3                 Criteria Gases                     NO2     42602   \n",
       "4                   Particulates      PM2.5 FRM/FEM Mass     88101   \n",
       "5                   Particulates  PM2.5 non FRM/FEM Mass     88502   \n",
       "6                   Particulates               PM10 Mass     81102   \n",
       "7                   Particulates                PMc Mass     86101   \n",
       "8                 Meteorological                   Winds      WIND   \n",
       "9                 Meteorological             Temperature      TEMP   \n",
       "10                Meteorological     Barometric Pressure     PRESS   \n",
       "11                Meteorological         RH and Dewpoint     RH_DP   \n",
       "12  Toxics, Precursors, and Lead                    HAPs      HAPS   \n",
       "13  Toxics, Precursors, and Lead                    VOCs      VOCS   \n",
       "14  Toxics, Precursors, and Lead                NONOxNOy  NONOxNOy   \n",
       "15  Toxics, Precursors, and Lead                    Lead      LEAD   \n",
       "\n",
       "                             Unit  Records  \n",
       "0               Parts per million     8420  \n",
       "1               Parts per billion     4253  \n",
       "2               Parts per million     6256  \n",
       "3               Parts per billion     5056  \n",
       "4     Micrograms/cubic meter (LC)    14924  \n",
       "5     Micrograms/cubic meter (LC)     7987  \n",
       "6   Micrograms/cubic meter (25 C)     4130  \n",
       "7     Micrograms/cubic meter (LC)     1255  \n",
       "8                           Knots     2192  \n",
       "9              Degrees Fahrenheit     1796  \n",
       "10                      Millibars       92  \n",
       "11      Percent relative humidity     1787  \n",
       "12    Micrograms/cubic meter (LC)     6536  \n",
       "13       Parts per billion Carbon     7433  \n",
       "14              Parts per billion    12028  \n",
       "15    Micrograms/cubic meter (LC)        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Download = False #Change to true if you want to download the data\n",
    "\n",
    "Download: \n",
    "    #ChatGPT 3.5 Turbo Code    \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import zipfile\n",
    "    from urllib.request import urlretrieve\n",
    "    \n",
    "    # 1. Define Attributes dictionary for air quality data\n",
    "    Attributes = {\n",
    "        'Category': ['Criteria Gases', 'Criteria Gases', 'Criteria Gases', 'Criteria Gases',\n",
    "                     'Particulates', 'Particulates', 'Particulates', 'Particulates',\n",
    "                     'Meteorological', 'Meteorological', 'Meteorological', 'Meteorological',\n",
    "                     'Toxics, Precursors, and Lead', 'Toxics, Precursors, and Lead',\n",
    "                     'Toxics, Precursors, and Lead', 'Toxics, Precursors, and Lead'],\n",
    "        'Parameter': ['Ozone', 'SO2', 'CO', 'NO2', 'PM2.5 FRM/FEM Mass', 'PM2.5 non FRM/FEM Mass',\n",
    "                      'PM10 Mass', 'PMc Mass', 'Winds', 'Temperature', 'Barometric Pressure',\n",
    "                      'RH and Dewpoint', 'HAPs', 'VOCs', 'NONOxNOy', 'Lead'],\n",
    "        'FileID': [44201, 42401, 42101, 42602, 88101, 88502, 81102, 86101, 'WIND', 'TEMP',\n",
    "                   'PRESS', 'RH_DP', 'HAPS', 'VOCS', 'NONOxNOy', 'LEAD'],\n",
    "        'Unit': ['na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na'],\n",
    "        'Records': [0] * 16\n",
    "    }\n",
    "    \n",
    "    # 2. Define region and CBSA_Name_Value\n",
    "    region = 'Miami'\n",
    "    CBSA_Name_Value = 'Miami-Fort Lauderdale-West Palm Beach, FL'\n",
    "    \n",
    "    # 3. Initialize download_count\n",
    "    download_count = 0\n",
    "    \n",
    "    # 4. Loop through each year and download files\n",
    "    for year in range(2019, 2022):\n",
    "        for i, file_id in enumerate(Attributes['FileID']):\n",
    "            filename = f'daily_{file_id}_{year}.zip'\n",
    "            base_url = 'https://aqs.epa.gov/aqsweb/airdata/'\n",
    "            download_from = base_url + filename\n",
    "            \n",
    "            # 5. Show download progress\n",
    "            download_count += 1\n",
    "            print(f\"Download {download_count}: {year} : {Attributes['Parameter'][i]} : {filename}\")\n",
    "            \n",
    "            # Download file\n",
    "            urlretrieve(download_from, filename)\n",
    "            \n",
    "            # 6. Unzip file\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall()\n",
    "            \n",
    "            # 7. Read csv file\n",
    "            raw_data = pd.read_csv(f'daily_{file_id}_{year}.csv', low_memory=False)\n",
    "            \n",
    "            # 8. Delete zip file if exists\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "            \n",
    "            # 9. Delete csv file if exists\n",
    "            if os.path.exists(f'daily_{file_id}_{year}.csv'):\n",
    "                os.remove(f'daily_{file_id}_{year}.csv')\n",
    "            \n",
    "            # 10. Handle warning in raw_data\n",
    "            pd.set_option('mode.chained_assignment', None)\n",
    "            \n",
    "            # 11. Update 'Unit' in Attributes\n",
    "            Attributes['Unit'][i] = raw_data['Units of Measure'][0] if len(raw_data) > 0 else 'empty_file'\n",
    "            \n",
    "            # 12. Filter by CBSA_Name_Value\n",
    "            raw_data = raw_data[raw_data['CBSA Name'] == CBSA_Name_Value]\n",
    "            \n",
    "            # 13. Keep required columns\n",
    "            raw_data = raw_data[['County Code', 'Parameter Name', 'Date Local', 'Units of Measure',\n",
    "                                 'Arithmetic Mean', 'CBSA Name']]\n",
    "            \n",
    "            # 14. Convert 'Date Local' to datetime and set as index\n",
    "            raw_data['Date Local'] = pd.to_datetime(raw_data['Date Local'])\n",
    "            raw_data.set_index('Date Local', inplace=True)\n",
    "            raw_data.sort_index(inplace=True)\n",
    "            \n",
    "            # 15. Update 'Records' in Attributes\n",
    "            Attributes['Records'][i] += len(raw_data)\n",
    "            \n",
    "            # 16. Save raw_data\n",
    "            save_folder = os.path.join('Data', 'L2_Structured')\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "            raw_data.to_csv(os.path.join(save_folder, f'daily_{file_id}_{year}_{region}.csv'))\n",
    "            \n",
    "    \n",
    "    # 17. Create DataFrame from Attributes\n",
    "    df_Attributes = pd.DataFrame(Attributes)\n",
    "    \n",
    "    # 18. Save df_Attributes\n",
    "    df_Attributes.to_csv(os.path.join(save_folder, 'Attributes.csv'))\n",
    "    \n",
    "    # 19. Display df_Attributes\n",
    "    display(df_Attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Data integration\n",
    "\n",
    "This step involves combining data from multiple data  to create a unified dataset for analysis.\n",
    "- We need to combine our data into a 2d array that has our 16 parameters (also known as features or variables) for the whole study period.\n",
    "\n",
    "We can give this code outline below for your LM to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'Miami'\n",
    "\n",
    "# read 'Data/L2_Structured/Attributes.csv' as `df_Attributes` DataFrame\n",
    "\n",
    "# create `data` DataFrame with daily index from 2019-01-01 to 2021-12-31 \n",
    "#and column names are the values of 'FileID' column in `df_Attributes`\n",
    " \n",
    "# loop from year 2019 to 2021\n",
    "    #Loop for each value in 'FileID'\n",
    "        #Read f'Data/L2_Structured/daily_{fileID}_{year}_{region}.csv' as `raw_data` DataFrame \n",
    "        #and set column 'Date Local' as datetime index\n",
    "\n",
    "       #resample the column 'Arithmetic Mean' in `raw_data` with .max() as `resampled_arithmetic_mean`\n",
    "\n",
    "       #Copy the values of resampled_arithmetic_mean for each fileID into its corresponding column in `data` with the same fileID\n",
    "       #Dates must match. For example, the value in 2019-01-01 in resampled_arithmetic_mean would be copied to \n",
    "       #the firt row of `data` with index equal 2019-01-01 \n",
    "\n",
    "# Save `data` DataFrame as 'data_{region}.csv at this path 'Data/L3_Integrated' and create folder if it does not exists\n",
    "\n",
    "#display `data` DataFrame using  `display()` not `print()`\n",
    "\n",
    "# Add rich code annotation\n",
    "\n",
    "#code only. no introduction and conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FileID</th>\n",
       "      <th>44201</th>\n",
       "      <th>42401</th>\n",
       "      <th>42101</th>\n",
       "      <th>42602</th>\n",
       "      <th>88101</th>\n",
       "      <th>88502</th>\n",
       "      <th>81102</th>\n",
       "      <th>86101</th>\n",
       "      <th>WIND</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRESS</th>\n",
       "      <th>RH_DP</th>\n",
       "      <th>HAPS</th>\n",
       "      <th>VOCS</th>\n",
       "      <th>NONOxNOy</th>\n",
       "      <th>LEAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>0.035412</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.477667</td>\n",
       "      <td>8.416667</td>\n",
       "      <td>7.158333</td>\n",
       "      <td>53.795833</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.291667</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.820833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.55</td>\n",
       "      <td>18.217647</td>\n",
       "      <td>6.404348</td>\n",
       "      <td>5.125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262.916667</td>\n",
       "      <td>74.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.958333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.576471</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>0.026882</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.56725</td>\n",
       "      <td>18.270833</td>\n",
       "      <td>9.929167</td>\n",
       "      <td>6.979167</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>307.708333</td>\n",
       "      <td>75.291667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.375</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.76</td>\n",
       "      <td>50.520833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.031882</td>\n",
       "      <td>0.740909</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>18.1625</td>\n",
       "      <td>12.4</td>\n",
       "      <td>10.7875</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.083333</td>\n",
       "      <td>72.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.6125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>0.034765</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>17.983333</td>\n",
       "      <td>12.016667</td>\n",
       "      <td>11.370833</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.166667</td>\n",
       "      <td>70.541667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.495833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>0.042882</td>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>24.579167</td>\n",
       "      <td>9.929167</td>\n",
       "      <td>8.045833</td>\n",
       "      <td>20</td>\n",
       "      <td>6.970833</td>\n",
       "      <td>202.208333</td>\n",
       "      <td>70.208333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.958333</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.8375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>0.040647</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>27.979167</td>\n",
       "      <td>8.865217</td>\n",
       "      <td>7.663158</td>\n",
       "      <td>21</td>\n",
       "      <td>6.904167</td>\n",
       "      <td>254.875</td>\n",
       "      <td>70.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>0.034059</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.665375</td>\n",
       "      <td>24.183333</td>\n",
       "      <td>8.183333</td>\n",
       "      <td>6.970833</td>\n",
       "      <td>17</td>\n",
       "      <td>7.8125</td>\n",
       "      <td>269.5</td>\n",
       "      <td>71.958333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>0.025706</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>23.5375</td>\n",
       "      <td>8.320833</td>\n",
       "      <td>6.708333</td>\n",
       "      <td>16</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>268.958333</td>\n",
       "      <td>73.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.75</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.466667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.717125</td>\n",
       "      <td>15.479167</td>\n",
       "      <td>12.591667</td>\n",
       "      <td>16.475</td>\n",
       "      <td>14</td>\n",
       "      <td>6.170833</td>\n",
       "      <td>261.416667</td>\n",
       "      <td>73.583333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.416667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.7125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "FileID         44201     42401     42101      42602      88101      88502  \\\n",
       "2019-01-01  0.035412    0.6625  0.477667   8.416667   7.158333  53.795833   \n",
       "2019-01-02  0.029471  0.754167      0.55  18.217647   6.404348      5.125   \n",
       "2019-01-03  0.026882     0.725   0.56725  18.270833   9.929167   6.979167   \n",
       "2019-01-04  0.031882  0.740909  0.533333    18.1625       12.4    10.7875   \n",
       "2019-01-05  0.034765  0.683333  0.408333  17.983333  12.016667  11.370833   \n",
       "...              ...       ...       ...        ...        ...        ...   \n",
       "2021-12-27  0.042882  0.805882    0.7375  24.579167   9.929167   8.045833   \n",
       "2021-12-28  0.040647  0.683333  0.816667  27.979167   8.865217   7.663158   \n",
       "2021-12-29  0.034059  0.691667  0.665375  24.183333   8.183333   6.970833   \n",
       "2021-12-30  0.025706  0.679167  0.804167    23.5375   8.320833   6.708333   \n",
       "2021-12-31   0.02925    0.5375  0.717125  15.479167  12.591667     16.475   \n",
       "\n",
       "FileID     81102     86101        WIND       TEMP PRESS      RH_DP   HAPS  \\\n",
       "2019-01-01  18.0       NaN  258.291667       73.0   NaN  86.833333    NaN   \n",
       "2019-01-02  18.0       NaN  262.916667      74.75   NaN  82.958333    NaN   \n",
       "2019-01-03  22.0      10.1  307.708333  75.291667   NaN     87.375   0.72   \n",
       "2019-01-04  28.0       NaN  155.083333     72.875   NaN  88.166667    NaN   \n",
       "2019-01-05  24.0       NaN  100.166667  70.541667   NaN      82.75    NaN   \n",
       "...          ...       ...         ...        ...   ...        ...    ...   \n",
       "2021-12-27    20  6.970833  202.208333  70.208333   NaN  84.958333   0.01   \n",
       "2021-12-28    21  6.904167     254.875  70.166667   NaN     87.625    NaN   \n",
       "2021-12-29    17    7.8125       269.5  71.958333   NaN      88.25    NaN   \n",
       "2021-12-30    16  7.066667  268.958333       73.5   NaN      89.75  0.007   \n",
       "2021-12-31    14  6.170833  261.416667  73.583333   NaN  88.416667    NaN   \n",
       "\n",
       "FileID      VOCS   NONOxNOy LEAD  \n",
       "2019-01-01   NaN  18.820833  NaN  \n",
       "2019-01-02   NaN  49.576471  NaN  \n",
       "2019-01-03  2.76  50.520833  NaN  \n",
       "2019-01-04   NaN    46.6125  NaN  \n",
       "2019-01-05   NaN  32.495833  NaN  \n",
       "...          ...        ...  ...  \n",
       "2021-12-27   NaN    42.8375  NaN  \n",
       "2021-12-28   NaN    85.0875  NaN  \n",
       "2021-12-29   NaN     77.915  NaN  \n",
       "2021-12-30   NaN  83.466667  NaN  \n",
       "2021-12-31   NaN    41.7125  NaN  \n",
       "\n",
       "[1096 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ChatGPT 3.5 Turbo Code \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "region = 'Miami'\n",
    "\n",
    "# Read 'Data/L2_Structured/Attributes.csv' as `df_Attributes` DataFrame\n",
    "df_Attributes = pd.read_csv('Data/L2_Structured/Attributes.csv')\n",
    "\n",
    "# Create `data` DataFrame with daily index from 2019-01-01 to 2021-12-31 \n",
    "# and with columns such that the column labels are the 'FileID' column in `df_Attributes`\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2021-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "data = pd.DataFrame(index=date_range, columns=df_Attributes['FileID'])\n",
    "\n",
    "# Loop from year 2019 to 2021\n",
    "for year in range(2019, 2022):\n",
    "    # Loop for each value in 'FileID'\n",
    "    for fileID in df_Attributes['FileID']:\n",
    "        # Read f'Data/L2_Structured/daily_{fileID}_{year}_{region}.csv' as `raw_data` DataFrame\n",
    "        file_path = f'Data/L2_Structured/daily_{fileID}_{year}_{region}.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            raw_data = pd.read_csv(file_path)\n",
    "            # Set column 'Date Local' as datetime index\n",
    "            raw_data['Date Local'] = pd.to_datetime(raw_data['Date Local'])\n",
    "            raw_data.set_index('Date Local', inplace=True)\n",
    "            \n",
    "            # Resample the column 'Arithmetic Mean' in `raw_data` with .max() as `resampled_arithmetic_mean`\n",
    "            resampled_arithmetic_mean = raw_data['Arithmetic Mean'].resample('D').max()\n",
    "            \n",
    "            # Copy the values of resampled_arithmetic_mean for each fileID into its corresponding column in `data`\n",
    "            # with the same fileID. Dates must match.\n",
    "            data.loc[resampled_arithmetic_mean.index, fileID] = resampled_arithmetic_mean.values\n",
    "\n",
    "# Save `data` DataFrame as 'data_{region}.csv' at this path 'Data/L3_Integrated' and create folder if it does not exist\n",
    "output_folder = 'Data/L3_Integrated'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "data.to_csv(os.path.join(output_folder, f'data_{region}.csv'))\n",
    "\n",
    "# Display `data` DataFrame using `display()`\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Data validation\n",
    "\n",
    "Data validation includes ensuring the accuracy, completeness, and reliability of the data.  Here we will simply cross-reference our collected data in `data` DataFrame with the data in raw data files to ensure that we did not do any mistake. We can display any rows and columns of your `data` DataFrame and cross-check them with raw data and the csv files. Here is one example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FileID</th>\n",
       "      <th>86101</th>\n",
       "      <th>WIND</th>\n",
       "      <th>TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>10.45</td>\n",
       "      <td>220.916667</td>\n",
       "      <td>75.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>6.995833</td>\n",
       "      <td>170.458333</td>\n",
       "      <td>71.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>6.233333</td>\n",
       "      <td>200.875</td>\n",
       "      <td>68.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "FileID         86101        WIND       TEMP\n",
       "2019-12-30     10.45  220.916667  75.833333\n",
       "2019-12-31  6.995833  170.458333  71.333333\n",
       "2020-01-01  6.233333     200.875  68.333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_show = ['86101', 'WIND','TEMP']\n",
    "data.loc['2019-12-30':'2020-01-01', columns_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also use the [JupyterLab variableInspector extension](https://github.com/jupyterlab-contrib/jupyterlab-variableInspector) to get an overview of the data before additional preprocessing:\n",
    "- review each parameter in the CSV file to ensure there are no errors.\n",
    "- note that the parameters \"NONOxNOy\", \"VOCs\", and \"HAPs\" need special handling due to its multiple-parameter nature.\n",
    "- acknowledge this issue for the current analysis. No further action is needed as this parameter won't be used in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Data enrichment\n",
    "\n",
    "This steps involved enhancing dataset with additional information from external sources to improve analysis. \n",
    "- No data enrichment is required at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Data transformation\n",
    "\n",
    "This step involves converting data into a format suitable form for analysis. Data enrichment and data transformation are closely related but distinct from data structuring. While data structuring primarily involves data wrangling for efficient storage and retrieval, data transformation focuses on preparing data for optimal analysis.\n",
    "\n",
    "Our analysis will focus on PM 2.5, PM 10, <NOsub>2</sub>, SO<sub>2</sub>, CO, and O<sub>3</sub>.\n",
    "| Parameter | FileID | Unit | Records |\n",
    "|---|---|---|---| \n",
    "PM2.5 FRM/FEM Mass\t| 88101\t| Micrograms/cubic meter (LC) |\t14924 |\n",
    "PM2.5 non FRM/FEM Mass |\t88502 |\tMicrograms/cubic meter (LC) |\t7987 |\n",
    "PM10 Mass |\t81102\t| Micrograms/cubic meter (25 C)\t| 4130 |\n",
    "NO2\t| 42602 |\tParts per billion\t| 5056 |\n",
    "SO2\t| 42401 |\tParts per billion\t| 4253 |\n",
    "CO\t| 42101 |\tParts per million\t| 6256 |\n",
    "Ozone\t| 44201\t| Parts per million\t| 8420 |\n",
    "Winds\t| WIND\t|Knots\t|2192|\n",
    "Temperature\t| TEMP |\tDegrees Fahrenheit |\t1796 |\n",
    "RH and Dewpoint |\tRH_DP |\tPercent relative humidity |\t1787 |\n",
    "HAPs |\tHAPS |\tMicrograms/cubic meter (LC) |\t6536 |\n",
    "VOCs |\tVOCS |\tParts per billion Carbon |\t7433 |\n",
    "\n",
    "To streamline this process, we will:\n",
    "- Combine 'PM2.5 FRM/FEM Mass' and 'PM2.5 non FRM/FEM Mass' for both samplers into one 'PM2.5' set.\n",
    "- Select parameters: PM2.5, PM10, NO2, SO2, CO, and Ozone.\n",
    "- Rename column labels from FileIDs to parameter names for clarity (e.g., '81102' to 'PM10').\n",
    "- Convert the DataFrame to a NumPy representation using `.values` for analysis with NumPy\n",
    "- Save the data as a NumPy array for analysis.\n",
    "\n",
    "We can ask our AI code assistant to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python code with rich annotations**\n",
    "\n",
    "region = 'Miami'\n",
    "\n",
    "1. Read 'data_{}.csv' as `data` DateFrame with datetime index from 'Data/L3_Integrated'\n",
    "2. In `data`, create column 'PM2.5' which is mean of columns '88101', '88502'\n",
    "3. Create a new DataFrame `aqi_data` as a copy of `data` which shall include the following columns from `data`\n",
    "```python\n",
    "columns_from_data= [\n",
    "'PM2.5', #PM2.5\n",
    "'81102', #PM10\n",
    "'42602', #NO2\n",
    "'42401', #SO2\n",
    "'42101', #CO\n",
    "'44201',  #O3\n",
    "]\n",
    "```\n",
    "4. In `aqi_data` DataFrame rename columns lables as follows:\n",
    "```\n",
    "'PM2.5': 'PM2.5'\n",
    "'81102', 'PM10'\n",
    "'42602', 'NO2'\n",
    "'42401', 'SO2'\n",
    "'42101', 'CO'\n",
    "'44201',  'O3'\n",
    "```\n",
    "5. In `air_quality_data`, get a NumPy representation of DataFrame using `.values` method\n",
    "6. To be later openned by NumPy, save the `aqi_data_` DataFrame  as 'aqi_data_{region}.csv' at 'Data/L4_Transformed' and create folder if it does not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "region = 'Miami'\n",
    "\n",
    "# 1. Read 'data_{}.csv' as `data` DataFrame with datetime index from 'Data/L3_Integrated'\n",
    "data = pd.read_csv(f'Data/L3_Integrated/data_{region}.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# 2. In `data`, create column 'PM2.5' which is mean of columns '88101', '88502'\n",
    "data['PM2.5'] = data[['88101', '88502']].mean(axis=1)\n",
    "\n",
    "# 3. Create a new DataFrame `aqi_data` as a copy of `data` which shall include specific columns\n",
    "columns_from_data = ['PM2.5', '81102', '42602', '42401', '42101', '44201']\n",
    "\n",
    "aqi_data = data[columns_from_data].copy()\n",
    "\n",
    "# 4. Rename columns labels in `aqi_data` DataFrame\n",
    "aqi_data.columns = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3']\n",
    "\n",
    "\n",
    "# 5. Get a NumPy representation of `aqi_data` DataFrame using `.values` method\n",
    "aqi_data_values = aqi_data.values\n",
    "\n",
    "# 6. Save the `aqi_data` DataFrame as 'aqi_data_{region}.csv' in 'Data/L4_Transformed' and create folder if it does not exist\n",
    "os.makedirs('Data/L4_Transformed', exist_ok=True)\n",
    "aqi_data.to_csv(f'Data/L4_Transformed/aqi_data_{region}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this transformation, our data is prepared for analysis. The next step is to utilize NumPy to calculate the air quality index before and after the lockdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "| Step(s)                        | File Name                            | Number of files | File size | Data size | Comments                              |\n",
    "|---------------------------------|--------------------------------------|-----------------|-----------|-----------|---------------------------------------|\n",
    "| 1) Data discovery and profiling    | 'daily_{FileID}_{year}.csv'              | 48              | ~60 MB    | ~ 3 GB    | 16 parameters and 3 years             |\n",
    "| 2) Data collection, cleaning, and structuring | 'daily_{FileID}_{year}_Miami.csv'  | 48            | ~0. 4MB   | ~ 10 MB  | 16 parameters and 3 years for Miami and selected columns            |\n",
    "|                                  | 'attributes.csv'                       | 1               |  2  KB       |          | Parameter details and file information |\n",
    "| 5) Data integration                 | 'data_Miami.csv'                                    | 1               | 125 KB         |  125 KB         | Integrated data with 16 parameters    |\n",
    "| 6) Data validation                  | 'data_Miami.csv'                                    |                |         |         | NONOxNOy, HAPs and VOCs columns need additional treatment |\n",
    "| 7) Data enrichment                  | 'data_Miami.csv'                                    |                |          |          | No data enrichment                              |\n",
    "| 8) Data transformation              | 'aqi_data_Miami.csv'                | 1               | 67 KB   | 67 KB    | Data for AQI analysis with NumPy representation |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
